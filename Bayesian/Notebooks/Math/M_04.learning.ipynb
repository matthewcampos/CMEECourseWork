{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to supervised machine learning\n",
    "\n",
    "In this session we will introduce the use of supervised machine learning on biological data.\n",
    "We will discuss nearest neighour classifier, support vector machine, neural networks and deep learning (specifically convolutional neural networks).\n",
    "\n",
    "Our toy example will be to classify species as endangered or not based genomic data. The rationale is that species with a small (effective population size) will have higher chances to be threatened. The amount of genomic variability (e.g. polymorphic sites and haplotype diversity) is taken as a proxy for the (effective) population size of each species. The genomic variation at marker loci is represented as an image.\n",
    "\n",
    "### Objective\n",
    "\n",
    "We assume that we have collected some genomic data on many species which have already been categorised into 4 classes of conservation status: \n",
    "1. least concern\n",
    "2. vulnerable\n",
    "3. endangered\n",
    "4. critically endagered. \n",
    "\n",
    "Our goal is to implement a classifier that given genomic data can predict whether a given species is endangered or not.\n",
    "\n",
    "More specifically, we assume we have **4 classes** of conservation status (labels: LC, VU, EN, CR) and **400 samples per class**. Each data point is an image which represents the (biallelic) genetic variation (binary data) across individuals (on the rows) over several genetic loci (on the columns). Images are double sorted to remove some noise associated to the order of samples and polymorphic sites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating images\n",
    "\n",
    "We will use images for our classification. To do so we need to learn how we can import and manipulate images in python. We will use `imageio` package to load and save images which are stored as `numpy` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume to load and manipulate an image of a cat and learn of this data is managed in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the directory accordingly\n",
    "img = imageio.imread(\"~/Documents/Teaching/statistical_inference/Slides/Learning/Pics/cat_generic.jpg\")\n",
    "\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are matrices with 3 dimensions. These are stored as `numpy` arrays with rank equal to 3. The third dimension is the colour channel. To being able to manipulate such objects we need to learn what `numpy` arrays are.\n",
    "\n",
    "### Numpy\n",
    "\n",
    "An array is a grid of values, all of the same type. It is indexed by a tuple of non-negative integers. The number of its dimensions is called *rank*, while the *shape* is a tuple of integers giving the size along each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3]) # rank=1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(a))\n",
    "\n",
    "print(a.shape)\n",
    "\n",
    "print(a[1])\n",
    "\n",
    "a[0] = 5\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[1,2,3],[4,5,6]]) # rank=2\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b.shape)\n",
    "b[0,0], b[0,1], b[1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION (1)***\n",
    "\n",
    "What is the shape of `img`, the `numpy` object containing the image of a cat? What is its data type (hint: use .dtype method)? How about its rank? (Possible solutions for DIYs in `Solutions/learning.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the rank and shape of `img` we may want to access some of its values. Let's see how we can perform array indexing in `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "print(a)\n",
    "\n",
    "# slicing\n",
    "b = a[:2,1:3]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a slice of an array is a view into the same data, so modifying it will modify the original array!\n",
    "print(a[0,1])\n",
    "b[0,0] = 100\n",
    "print(a[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION (2)*** \n",
    "\n",
    "Extract the top left corner of the cat image (e.g. 200x200) and write a new image. Hint: you can use `imageio.imwrite` which takes the file name to save and the `numpy` object.\n",
    "What happens if you just take one slice of the third dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageio.imwrite(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.imwrite(\"~/Downloads/cat_slice.jpg\", img[:200, :200, :])\n",
    "imageio.imwrite(\"~/Downloads/cat_grey.jpg\", img[:, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to access values which satisfy a condition we set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],[3,4],[5,6]])\n",
    "print(a > 2)\n",
    "print(a[a > 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily do mathematical operations involving matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2],[3,4]], dtype=np.float32) # notice that we force the data type to be floats\n",
    "y = np.array([[5,6],[7,8]], dtype=np.float32)\n",
    "\n",
    "print(x + y)\n",
    "print(np.add(x, y))\n",
    "\n",
    "print(x - y)\n",
    "#np.subtract(x, y)\n",
    "\n",
    "print(x * y) # elementwise!\n",
    "#np.multiply(x, y)\n",
    "\n",
    "print(x / y) # elementwise!\n",
    "#np.divide(x, y)\n",
    "\n",
    "print(np.sqrt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2],[3,4]])\n",
    "print(x)\n",
    "print(x.sum())\n",
    "\n",
    "print(x.sum(axis=0))\n",
    "print(x.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION (3)*** \n",
    "\n",
    "What happens if we substract the cat image from the cat image? What happens if we add them? Write such images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last think to mention about `numpy` arrays is how we can manipulate them using broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply a matrix by a constant\n",
    "x = np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
    "print(x * 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
    "y = np.array([1,0,1])\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "z = x + y\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION (4)*** \n",
    "\n",
    "Create a tinted image of the cat, by multiplying its colour channel by `[1, 0.95, 0.9]`. Instead of writing a new file, plot it on the screen. We use the package `matplotlib` and its function `imshow` which takes the `numpy` array in input. You need to force the data type to be unsigned integer 8 bits and you can achieve this with the function `np.uint8()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ...\n",
    "# plt.imshow(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing data\n",
    "\n",
    "The first think we do is to load images representing genomic data from various species. These images have already been converted into `numpy` arrays. (Note that we have already preprocessed our data, so pixel intensitities have been scaled between 0 and 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"Data/X_train.npy\")\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to plot one image, then we need to do a proper slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0,:,:,0].shape)\n",
    "\n",
    "plt.imshow(X_train[0,:,:,0], cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which corresponds to the following matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0,:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the data itself (called `X`), we also need labels (called `y`) associated to each data point. These labels indicate which class each data point belongs to. What is the rank and shape of this `numpy` array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load(\"Data/y_train.npy\")\n",
    "print(y_train.shape)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall, we collected 400 samples per class so we should expect 1600 data points. We have 1200 entries in the training set. Why?\n",
    "\n",
    "We split the data we have into training and testing. The learning will be done in the training set and the measurement of accuracy will happen on the testing set. What is the rank and shape of the these `numpy` objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"Data/X_test.npy\")\n",
    "print(X_test.shape)\n",
    "\n",
    "y_test = np.load(\"Data/y_test.npy\")\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to predict the classification of an unknown species, the Marsican bear (labelled as `ursus`).\n",
    "Let's load its image, check its rank and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ursus = np.load(\"Data/X_ursus.npy\")\n",
    "print(X_ursus.shape)\n",
    "\n",
    "plt.imshow(X_ursus[0,:,:,0], cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbour Classifier\n",
    "\n",
    "\n",
    "Now we wan to implement a NN classifier to assign a label to our image of the Marsican bear.\n",
    "The idea is to select the image(s) which is (are) closer to the one of interest.\n",
    "For instance, the simple elementwise difference between images can be calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ursus[0,:,:,0] - X_train[0,:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION (5)*** Implement a NN classifier using the L1 distance and predict the label for the Ursus image. A possible framework is given below but feel free to use your own creativity. (A possible solution is given in in `Solutions/learning.py`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = ['LC', 'VU', 'EN', 'CR'] # these are the 4 possible classes\n",
    "min_distance = 9999 # initialise a value for the distance\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    distance = ...\n",
    "    if ...\n",
    "        ...\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can easily implement a NN classifer applied to the whole testing set. What's the achieved accuracy? You can do it yourself, but the idea is that you have to execute the above operation for each sample from the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow / keras\n",
    "\n",
    "We will be using TensorFlow to build and train our deep neural network. In particular, we will use Keras which is TensorFlow's high-level API.\n",
    "Keras is used for fast prototyping and production since it is\n",
    "* User friendly: a simple, consistent interface optimized for common use cases.\n",
    "* Modular and composable: models are made by connecting building blocks together.\n",
    "* Easy to extend: you can write custom building blocks, new layers, loss functions, etc. etc.\n",
    "\n",
    "`tensorflow.keras` is the implementation of [Keras API specification](https://keras.io/). `tf.keras` can run any Keras-compatible code. In this practical, we will just focus on using `keras` directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential model\n",
    "\n",
    "Let's build a simple model. In Keras, you assemble layers to build models. A model is a graph of layers. The most common type of model is a stack of layers: the `keras.Sequential` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, we can build a simple, fully-connected network with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "# Adds a densely-connected layer with 16 units to the model:\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "# Add another:\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the layers\n",
    "\n",
    "There are several `keras.layers` available with some common constructor parameters:\n",
    "* activation: set the activation function for the layer, specified by the name of a built-in function or as a callable object.\n",
    "* kernel_initializer and bias_initializer: the initialization schemes that create the layer's weights (kernel and bias).\n",
    "* kernel_regularizer and bias_regularizer: the regularization schemes that apply the layer's weights (kernel and bias).\n",
    "The following examples show how to create instances of `keras.layers.Dense` layers using constructor arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import activations, regularizers, initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sigmoid layer:\n",
    "layers.Dense(16, activation='sigmoid')\n",
    "# Or:\n",
    "layers.Dense(16, activation=activations.sigmoid)\n",
    "\n",
    "# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:\n",
    "layers.Dense(16, kernel_regularizer=regularizers.l1(0.01))\n",
    "\n",
    "# A linear layer with L2 regularization of factor 0.01 applied to the bias vector:\n",
    "layers.Dense(16, bias_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "# A linear layer with a kernel initialized to a random orthogonal matrix:\n",
    "layers.Dense(16, kernel_initializer='orthogonal')\n",
    "\n",
    "# A linear layer with a bias vector initialized to 2.0s:\n",
    "layers.Dense(16, bias_initializer=initializers.constant(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "After the model is constructed, we can configure its learning process by calling the `compile` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "                        # Adds a densely-connected layer with 16 units to the model: \n",
    "                        layers.Dense(16, activation='relu'),\n",
    "                        # Add another:\n",
    "                        layers.Dense(16, activation='relu'),\n",
    "                        # Add a softmax layer with 5 output units:\n",
    "                        layers.Dense(5, activation='softmax')])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.compile` takes three main arguments:\n",
    "* optimizer: it specifies the training procedure, such as `Adam`, `RMSProp`, or `GradientDescent`.\n",
    "* loss: function to minimize during optimization, such as mean square error (mse), categorical_crossentropy, and binary_crossentropy, specified by name or by passing a callable object from the `keras.losses` module.\n",
    "* metrics: to monitor training; string names or callables from the `keras.metrics` module.\n",
    "\n",
    "Let's make a quick example using `numpy` to train and evaluate our model using the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 5))\n",
    "\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.fit` takes three main arguments:\n",
    "* epochs: training is structured into epochs, iterations over the entire input data.\n",
    "* batch_size: the model slices the data into smaller batches and iterates over these batches during training.\n",
    "* validation_data: tuple of inputs and labels for validation.\n",
    "\n",
    "When prototyping a model, you want to easily monitor its performance on some validation data. Passing the validation_data argument allows the model to display the loss and metrics in inference mode for the passed data, at the end of each epoch. Here's an example using validation_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 5))\n",
    "\n",
    "val_data = np.random.random((100, 32))\n",
    "val_labels = np.random.random((100, 5))\n",
    "\n",
    "model.fit(data, labels, epochs=10, batch_size=32,\n",
    "          validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large datasets or multi-device training you can use the [Datasets API](https://www.tensorflow.org/guide/datasets).\n",
    "\n",
    "### Evaluate and predict\n",
    "\n",
    "The `model.evaluate` and `model.predict` methods can use `numpy` data (and/or a `tensorflow.data.Dataset`) to evaluate the inference-mode loss and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 5))\n",
    "\n",
    "model.evaluate(data, labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict the output for the data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(data, batch_size=32)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read the full [documentation](https://www.tensorflow.org/guide/keras) for building custom models and more advanced models. \n",
    "You may also want to check out [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) and [Low Level](https://www.tensorflow.org/guide/low_level_intro) information which is helpful for debugging.\n",
    "A series of [tutorials](https://www.tensorflow.org/tutorials/) is also available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting conservation status from genomic data\n",
    "\n",
    "Let's build our convnet. First thing, we need to define the architecture.\n",
    "To do that, we need to import some modules from `keras`, as previously seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models, layers, activations, optimizers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"Data/X_train.npy\")\n",
    "y_train = np.load(\"Data/y_train.npy\")\n",
    "\n",
    "X_test = np.load(\"Data/X_test.npy\")\n",
    "y_test = np.load(\"Data/y_test.npy\")\n",
    "\n",
    "X_ursus = np.load(\"Data/X_ursus.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first example, we will just do a binary classification into either 'LC' or 'VU' classes for a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "y2_train = np.zeros(y_train.shape[0], dtype='float32')\n",
    "y2_train[np.where(np.argmax(y_train, axis=1) < 2)[0]] = 1.\n",
    "# testing data\n",
    "y2_test = np.zeros(y_test.shape[0], dtype='float32')\n",
    "y2_test[np.where(np.argmax(y_test, axis=1) < 2)[0]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "Building the neural network requires configuring the layers of the model, then compiling the model.\n",
    "The sequential model type from `keras` is simply a linear stack of neural network layers.\n",
    "Each layer will be added (or stacked) to the initial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model needs also to know what input shape it should expect. The first layer (but only the first) needs to receive such information.\n",
    "Let's add a convolutional layer. We can read how to do it from [here](https://keras.io/layers/convolutional/#conv2d).\n",
    "Also note that we can add activation and padding layers directly into this convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), activation=\"relu\", padding=\"same\", input_shape=(64, 64, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a convolutional layer it is often used a max-pooling layer. Read its definition [here](https://keras.io/layers/pooling/#maxpooling2d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add several cycles of Conv-MaxPool-Dropout.\n",
    "If you then want to move towards the final fully connected neural network, you need to first flatten the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can add a fully connected network with a relu activation using a [Dense](https://keras.io/layers/core/#dense) layer, another dropout layer, and then the output layer with a softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(units=32, activation=\"relu\"))\n",
    "# we need 1 units at the output since it is a binary classification, also the final activation is 'sigmoid'\n",
    "model.add(layers.Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even print a summary of our architecture with the specification of the learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling\n",
    "\n",
    "We need to \"compile\" the network and prepare it for the training. We do it by specifying the [loss function](https://keras.io/losses/) and [optimisation](https://keras.io/optimizers/) to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Let's train our network. We pass the training data set and the network will optimise its parameters to minimise the loss function.\n",
    "We can allocate a portion of the training data as validation data. You can read more [here](https://keras.io/models/sequential/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y2_train, batch_size=32, epochs=10, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy is used to tune the hyper-parameters (e.g. learning rate, dropout rate).\n",
    "It's convenient to plot the decay of loss and increase of accuracy for both the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "\n",
    "train_loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "train_acc = hist.history['acc']\n",
    "val_acc = hist.history['val_acc']\n",
    "\n",
    "xc = range(10)\n",
    "x_axis = np.zeros(len(xc))\n",
    "for x,i in enumerate (xc):\n",
    "    x_axis[i] = x + 1\n",
    "\n",
    "rcParams['axes.titlepad'] = 20 \n",
    "plt.figure(1,figsize=(7,5),facecolor='white')\n",
    "plt.plot(x_axis,train_loss)\n",
    "plt.plot(x_axis,val_loss)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training loss and validation loss',fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.legend(['Training loss','Validation loss'],fontsize=12)\n",
    "plt.style.use(['classic'])\n",
    "\n",
    "plt.figure(2,figsize=(7,5),facecolor='white')\n",
    "plt.plot(x_axis,train_acc)\n",
    "plt.plot(x_axis,val_acc)\n",
    "plt.xlabel('Epoch',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12) \n",
    "plt.title('Training accuracy and validation accuracy',fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.legend(['Training accuracy','Validation accuracy'],fontsize=12,loc=4)\n",
    "plt.style.use(['classic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Now we need to test our network. In other words we evaluate it using the testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y2_test)\n",
    "print(score) # will return the test loss and test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even plot a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# if you don't have sklearn install it with: conda install scikit-learn\n",
    "\n",
    "y2_pred = np.where(model.predict(X_test, batch_size=None, verbose=1)<0.5,0.,1.)\n",
    "\n",
    "cm = confusion_matrix(y2_test, y2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['low', 'high'] # these are the 2 labels\n",
    "np.set_printoptions(precision=2)\n",
    "fig = plt.figure(facecolor='white')\n",
    "title = 'Normalized confusion matrix'\n",
    "cmap = plt.cm.Blues\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "plt.title(title)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=90, fontsize=8)\n",
    "#plt.xticks(tick_marks, rotation=45, fontsize=6) # alternative\n",
    "plt.yticks(tick_marks, classes, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Finally we used the final optimised network to predict the label for unknown entries.\n",
    "More info [here](https://keras.io/models/sequential/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ursus = model.predict(X_ursus)\n",
    "print(y_ursus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing is that these are proper posterior probabilities. Therefore we can even calculate Bayes factors and do model testing. Imagine that we want to test whether this species is threatened (so not \"least concern\" or \"vulnerable\"). We can calculate this Bayes factor as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_ursus[0]) / (1 - y_ursus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BYON (Build Your Own Network)\n",
    "\n",
    "Based on the previous templare, build and train a network to predict the 4 different classes of conservation status.\n",
    "Build your own architecture and compiler using the full data set (`X_train` and `y_train`).\n",
    "Test the accuracy on the testing data set.\n",
    "Finally, predict the conservation status of `X_ursus`.\n",
    "\n",
    "You need to think carefully about your final layer(s), loss function and validation metrics.\n",
    "metrics.\n",
    "Also, be aware of various tensor sizes and make sure they are correct in your layers and in your predictions.\n",
    "\n",
    "Some general tips:\n",
    "- you can use multiple cycles of Conv+MaxPool layers\n",
    "- you can play with the dropout rates and/or regularizers to prevent overfitting\n",
    "- you can tune the learning rate\n",
    "- you can add more/less units (or neurons) in the dense layers\n",
    "- you can add more/less filters or change their size\n",
    "- try a leakyReLu or other activation functions\n",
    "- change how you initialise the weights\n",
    "- ...\n",
    "\n",
    "Be aware that the accuracy won't improve by default if you make the network deeper: you have more parameters to optimise with the same data set! It's always a good idea to check the number of parameters to estimate (and size of each layer). Do not overcomplicate things!\n",
    "\n",
    "Ideally, you may want to try different configurations and retain the one with the highest validation accuracy. Then this network will be passed to the testing set. For instance, assume that you have one hyper-parameter and you want to estimate it. How would you build such pipeline?\n",
    "\n",
    "The best way of learning is by doing. Read the keras manual https://keras.io/ to learn how to implement your ideas.\n",
    "Pick a partner or more and start a team.\n",
    "\n",
    "It's a competition.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
